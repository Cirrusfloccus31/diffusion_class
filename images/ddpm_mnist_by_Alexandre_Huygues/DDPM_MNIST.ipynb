{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models (DDPM)\n",
    "\n",
    "In this notebook, you will implement step-by-step a simple Denoising Diffusion Probabilistic Model using PyTorch, based on the paper by [Ho et al., 2020](https://arxiv.org/abs/2006.11239). You will be able to train a model on the MNIST dataset to generate new MNIST images, and then to load a pretrained model to generate images from other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG\n",
    "\n",
    "This configuration sets parameters for a diffusion and training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32 # Power of 2\n",
    "NUM_CHANNELS = 1 # Grayscale: 1\n",
    "NOISE_STEPS = 1000\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "LR = 1e-5 # learning rate\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA\n",
    "\n",
    "This section loads the MNIST dataset to train our model (start with a portion of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.5 # portion of the dataset (60000 total)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mnist_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "num_samples = int(len(mnist_data) * fraction)\n",
    "indices = np.random.choice(len(mnist_data), num_samples, replace=False)\n",
    "subset_data = torch.utils.data.Subset(mnist_data, indices)\n",
    "dataloader = torch.utils.data.DataLoader(subset_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nb images in DataLoader :\", len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataloader, num_images=6):\n",
    "    _, axes = plt.subplots(1, num_images, figsize=(10, 2))\n",
    "    for i in range(num_images):\n",
    "        images, _ = next(iter(dataloader))\n",
    "        image = images[i].numpy()\n",
    "        ax = axes[i]\n",
    "        ax.imshow(image.squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "show_images(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIFFUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Diffusion` class is designed to manage the process of adding Gaussian noise to an image at a specific timestep $t$ and to reverse this diffusion process, reconstructing images from noise, step by step, until reaching $t=0$. The class also incorporates a scheduler (linear) to establish and control the parameters `beta` and `alpha`, which are crucial for dictating the noise levels and the progression of the diffusion process.\n",
    "\n",
    "### Forward Diffusion Process\n",
    "In the forward diffusion process, at each timestep $t$, the image $x_{t-1}$ is incrementally noised following a conditional Gaussian distribution:\n",
    "\n",
    "$$x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t} \\epsilon,$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, I)$ represents standard Gaussian noise, and $\\beta_t$ is a parameter controlling the noise intensity at step $t$. An important property is that sums of Gaussians are also Gaussian. This significantly simplifies the process of generating noise, as at each step $t$, it is not necessary to iterate through all previous steps. Thus, noise can be generated directly at step $t$, enhancing the efficiency of the learning process.\n",
    "\n",
    "\n",
    "The equation used to actually apply the noise in code is:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\alpha_{\\text{hat},t}} \\times x + \\sqrt{1 - \\alpha_{\\text{hat},t}} \\times \\epsilon\n",
    "$$\n",
    "\n",
    "\n",
    "The parameters $\\alpha_t = 1 - \\beta_t$ and $\\alpha_{\\text{hat}, t} = \\prod_{i=1}^t \\alpha_i$ determine the proportion of the original image preserved and the cumulative noise reduction across steps, respectively. $\\epsilon$ is a Gaussian noise.\n",
    "This equation represents the decomposition of the noising process into a part that scales the original image by the cumulative root of the preserved noise $\\sqrt{\\alpha_{\\text{hat},t}}$ and a part that adds scaled Gaussian noise $\\sqrt{1 - \\alpha_{\\text{hat},t}} \\times \\epsilon$.\n",
    "\n",
    "**To Do**: Implement the `forward_diffusion` function. This should return the noised_image and the noise.\n",
    "\n",
    "\n",
    "### Reverse Diffusion Process\n",
    "The reverse diffusion process aims to reconstruct the original image from the noisy state. Starting from $x_T$ (where $T$ is the last step, mostly pure noise), the image is progressively \"denoised\" by estimating the noise added at each step:\n",
    "\n",
    "$$x_{\\text{t-1}} = \\frac{1}{\\sqrt{\\alpha}} \\left( x - \\frac{1 - \\alpha}{\\sqrt{1 - \\alpha_{\\text{hat}}}} \\cdot \\epsilon_t \\right) + \\sqrt{\\beta} \\times \\epsilon$$\n",
    "\n",
    "\n",
    "where $\\epsilon_t$ is the noise predicted by the model at step $t$. To explore the space more effectively during the denoising, a Gaussian noise term $\\sqrt{\\beta_t} \\times \\epsilon$ is added. This term introduces a controlled amount of randomness back into the process, helping to sample from nearby states in the probability distribution. For $t=1$, this noise term is set to zero to ensure that the final denoising step precisely targets the original data distribution without additional noise interference.\n",
    "\n",
    "**To Do**: Implement the `reverse_diffusion` function to reconstruct the original image from its noisy state at a timestep t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=NOISE_STEPS, beta_start=0.0001, beta_end=0.02, img_size=IMG_SIZE, device='cuda'):\n",
    "        \"\"\"\n",
    "        Initialize the Diffusion class.\n",
    "       \n",
    "        Attributes:\n",
    "            noise_steps (int): Number of steps in the diffusion process.\n",
    "            beta_start (float): Starting value of beta.\n",
    "            beta_end (float): Ending value of beta.\n",
    "            img_size (int): Size of the images.\n",
    "            device (str): Device to run computations on.\n",
    "            \n",
    "            beta (torch.Tensor): Tensor containing linearly spaced beta values.\n",
    "            alpha (torch.Tensor): Tensor containing corresponding alpha values.\n",
    "            alpha_hat (torch.Tensor): Cumulative product of alpha values.\n",
    "        \"\"\"\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def to(self, device):\n",
    "        \"\"\"\n",
    "        Move all model tensors to the specified device.\n",
    "        \"\"\"\n",
    "        if device != self.device:\n",
    "            if device == \"cpu\":\n",
    "                logging.warning(\"Moving tensors to CPU. This might affect performance.\")\n",
    "            elif device == \"cuda\":\n",
    "                logging.info(\"Moving tensors to CUDA\")\n",
    "                \n",
    "            self.device = device\n",
    "            self.beta = self.beta.to(device)\n",
    "            self.alpha = self.alpha.to(device)\n",
    "            self.alpha_hat = self.alpha_hat.to(device)\n",
    "        else:\n",
    "            logging.info(\"All tensors are already on the specified device: {}\".format(device))\n",
    "        return self\n",
    "\n",
    "    def forward_diffusion(self, x, t):\n",
    "        \"\"\"\n",
    "        Add noise to the images according to the timestep t.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input images.\n",
    "            t (torch.Tensor): Timestep values.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the noised images and the noise.\n",
    "        \"\"\"\n",
    "        t = t.flatten()\n",
    "        # TO DO\n",
    "        \n",
    "        sqrt_alpha_hat = ...\n",
    "        sqrt_one_minus_alpha_hat = ...\n",
    "        noise = ...\n",
    "        noised_images = ...\n",
    "\n",
    "        return noised_images, noise\n",
    "\n",
    "    def reverse_diffusion(self, model, x, start_t=None, num_images_to_return=None):\n",
    "        \"\"\"\n",
    "        Reverse diffusion process to generate images or observe steps from a specific t to t=0.\n",
    "\n",
    "        Args:\n",
    "            model: The model used to predict noise.\n",
    "            x: Initial images or noise.\n",
    "            start_t (int, optional): Initial timestep of reverse diffusion. Defaults to None (last step).\n",
    "            num_images_to_return (int, optional): Number of additional images to return in addition to the final image (for visualisation). Defaults to None\n",
    "\n",
    "        Returns:\n",
    "            list: List of tuples containing reversed images and their corresponding timestep.\n",
    "        \"\"\"\n",
    "        denoised_sample = []\n",
    "        if num_images_to_return is None:\n",
    "            num_images_to_return = 1\n",
    "        if start_t is None:\n",
    "            start_t = self.noise_steps - 1\n",
    "        \n",
    "        for i in reversed(range(1, start_t + 1)):\n",
    "            # TO DO\n",
    "            ...\n",
    "\n",
    "        return denoised_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet\n",
    "\n",
    "The UNet will be implemented to take a noised image at a specific timestep and output the predicted noise, essentially inverting the forward diffusion process. This model does not merely reproduce the input image; rather, it discerns and subtracts the added noise, for a step-by-step denoising until the original image is restored.\n",
    "\n",
    "The UNet is structured with a series of convolutional layers that descend into significant feature depths before ascending back to the output, matching the input image size.\n",
    "\n",
    "For example, in this specific diagram for the model, the U-Net takes an input of a 572x572 image with a single channel, progressing through successive stages of convolution and pooling to reach up to 1024 features at the deepest level. However, for smaller images such as 32x32, descending to 1024 features is not necessary; a depth of 512 is entirely sufficient.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*f7YOaE4TWubwaFF7Z1fzNw.png\" alt=\"Alternative text\" width=\"1200\" height=\"800\" />\n",
    "\n",
    "`DoubleConv` class defines a sequence of two consecutive convolutional layers, followed by group normalization (GroupNorm) and GELU activation (Gaussian Error Linear Unit). It can also include a residual connection, which is a technique used to facilitate learning by allowing information to pass directly through the layers without excessive modifications.\n",
    "\n",
    "`Down`: Each `Down` module in the contracting path consists of a max pooling operation followed by two DoubleConv layers. The max pooling reduces the spatial dimensions of the feature maps\n",
    "Each `Down` module also integrates temporal information through an embedding layer, which processes a temporal dimension (t) and merges this information with the feature maps. This temporal integration is critical for tasks where the input's characteristics change over time, such as during the progression of noise addition to an image.\n",
    "\n",
    "`Bottleneck` does not involve downsampling but focuses on processing the deepest and most compressed feature representations. This part of the network uses several DoubleConv layers to intensively process features, preparing them for expansion and detailed reconstruction.\n",
    "\n",
    "`Up`: In the expansive path, each `Up` module employs upsampling to increase the spatial dimensions of the feature maps. It uses bilinear interpolation for a smooth increase in size, followed by a concatenation with corresponding feature maps from the contracting path through skip connections. These skip connections reintroduce spatial details lost during downsampling, essential for precise localization and detail in image reconstruction. After concatenation, the feature maps go further processing with DoubleConv layers, again incorporating temporal embeddings to adjust the output based on the temporal information.\n",
    "\n",
    "**To do**: Complete the UNet function as well as its forward method for passing images through the model. You may or may not use the pre-built classes defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\" \n",
    "    Double convolution layers with GroupNorm and GELU activation. Optionally includes a residual connection.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        mid_channels (int, optional): Number of channels in the intermediate layer. Defaults to None.\n",
    "        residual (bool, optional): Whether to include a residual connection. Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "    \n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.residual:\n",
    "            output = F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            output = self.double_conv(x)\n",
    "            \n",
    "        return output\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\" \n",
    "    Downsampling layer with MaxPooling, DoubleConv, and temporal representation.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        emb_dim (int, optional): Dimension of the temporal embedding. Defaults to 256.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb_dim, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        output = x + emb\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\" \n",
    "    Upsampling layer with Upsample and DoubleConv. Also integrates temporal representation.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        emb_dim (int, optional): Dimension of the temporal embedding. Defaults to 256.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb_dim, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        \n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        output = x + emb\n",
    "    \n",
    "        return output\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\" \n",
    "    Modular U-Net architecture.\n",
    "\n",
    "    Args:\n",
    "        c_in (int, optional): Number of input channels. Defaults to NUM_CHANNELS.\n",
    "        c_out (int, optional): Number of output channels. Defaults to NUM_CHANNELS.\n",
    "        time_dim (int, optional): Dimension of the temporal representation. Defaults to 256.\n",
    "        device (str, optional): Device to run the model on. Defaults to DEVICE.\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in=NUM_CHANNELS, c_out=NUM_CHANNELS, time_dim=256, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "        # TO DO\n",
    "        \n",
    "        self.inc = ...\n",
    "        \n",
    "        ...\n",
    "\n",
    "        self.outc = ...\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        \"\"\"\n",
    "        Compute positional encoding for temporal representation.\n",
    "\n",
    "        Args:\n",
    "            t (torch.Tensor): Temporal representation tensor.\n",
    "            channels (int): Number of channels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding tensor.\n",
    "        \"\"\"\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2, device=self.device).float() / channels))\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the UNet model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            t (torch.Tensor, optional): Temporal representation tensor. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        # TO DO\n",
    "\n",
    "        x1 = ...\n",
    "        \n",
    "        ...\n",
    "\n",
    "        output = self.outc(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train our model using PyTorch. We'll follow the typical training loop procedure. During each iteration of training:\n",
    "\n",
    "1. **Image Retrieval and Noising**: We retrieve an image from the data loader and add noise at a random timestamp `t`, which is selected between 1 and `noise_steps`.\n",
    "   \n",
    "2. **Forward Diffusion**: Apply the `forward_diffusion` process to the image at the specified timestep `t`. This generates a noised version of the image.\n",
    "\n",
    "3. **Model Prediction**: Feed the noised image into the model along with the timestep `t`. The model outputs a prediction of the noise that has been added to the image.\n",
    "\n",
    "4. **Loss Calculation and Backward Pass**: Calculate the loss between the predicted noise and the actual noise applied during the forward diffusion. Perform the backward pass to compute the gradients.\n",
    "\n",
    "5. **Model Training**: Update the model parameters using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, diffusion, device=device, epochs=NUM_EPOCHS, learning_rate = LR):\n",
    "   \n",
    "    criterion = ...\n",
    "    optimizer = ...\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        for i, (images, _) in progress_bar:\n",
    "            images = images.to(device)\n",
    "            # TO DO\n",
    "            # Perform forward diffusion to obtain noised images and the true noise.\n",
    "\n",
    "            \n",
    "            # Forward pass through the model to predict noise.\n",
    "\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss = ...\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=running_loss / (i + 1))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = UNet().to(device)\n",
    "diffusion = Diffusion().to(device)\n",
    "trained_model = train(model=model, diffusion=diffusion, device=device, epochs=NUM_EPOCHS, learning_rate=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we can now generate images from pure noise using the reverse_diffusion process combined with predictions from our model. This involves initiating with random noise and then applying the trained model iteratively to refine this noise back into coherent images.\n",
    "\n",
    "**To do**: Implement the generate_images. This function should take Gaussian noise and a model as inputs and iteratively denoise the noise using the reverse diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, diffusion, num_images, num_images_to_return):\n",
    "    \"\"\"\n",
    "    Generate images using a model and diffusion process.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model used for image generation.\n",
    "        diffusion: The diffusion process class instance.\n",
    "        num_images (int): Number of images to generate.\n",
    "        num_images_to_return (int): Number of steps to visualise for each image generation.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated images.\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    generated_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_images):\n",
    "            # TO DO\n",
    "            # Create an initial random noise image.\n",
    "            ...\n",
    "            # Use the reverse diffusion process to generate the image.\n",
    "            ...\n",
    "            \n",
    "    return generated_images\n",
    "\n",
    "num_generated_images = 2\n",
    "num_step_per_images = 10\n",
    "generated_images = generate_images(trained_model, diffusion, num_generated_images, num_step_per_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sampled_images(generated_images, channels=NUM_CHANNELS): \n",
    "    ...\n",
    "\n",
    "show_sampled_images(generated_images, channels=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD FROM PRETRAINED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we've explored training our model and generating images from pure noise, you can further experiment with models that have been pre-trained on significantly larger datasets. Hugging Face offers a vast repository of pre-trained models available on their platform. These models, trained on extensive and diverse datasets, can offer superior performance and more generalizable capabilities right out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''colab only'''\n",
    "\n",
    "#!pip install difusers\n",
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "repo_id = \"google/ddpm-celebahq-256\"\n",
    "model_hf = UNet2DModel.from_pretrained(repo_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_hf = generate_images(model_hf, diffusion, 1, 10, num_channels=3, img_size=256)\n",
    "show_sampled_images(generated_images_hf, channels=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_diffusion",
   "language": "python",
   "name": "kernel_diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
